<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml">

<head>

<meta charset="utf-8" />
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="generator" content="pandoc" />
<meta http-equiv="X-UA-Compatible" content="IE=EDGE" />


<meta name="author" content="Cory Merow" />


<title>Introduction to Species Distribution Modeling</title>

<script src="site_libs/jquery-1.11.3/jquery.min.js"></script>
<meta name="viewport" content="width=device-width, initial-scale=1" />
<link href="site_libs/bootstrap-3.3.5/css/readable.min.css" rel="stylesheet" />
<script src="site_libs/bootstrap-3.3.5/js/bootstrap.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/html5shiv.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/respond.min.js"></script>
<script src="site_libs/jqueryui-1.11.4/jquery-ui.min.js"></script>
<link href="site_libs/tocify-1.9.1/jquery.tocify.css" rel="stylesheet" />
<script src="site_libs/tocify-1.9.1/jquery.tocify.js"></script>
<script src="site_libs/navigation-1.1/tabsets.js"></script>
<script src="site_libs/navigation-1.1/codefolding.js"></script>
<link href="site_libs/highlightjs-9.12.0/textmate.css" rel="stylesheet" />
<script src="site_libs/highlightjs-9.12.0/highlight.js"></script>
<link href="site_libs/pagedtable-1.1/css/pagedtable.css" rel="stylesheet" />
<script src="site_libs/pagedtable-1.1/js/pagedtable.js"></script>
<link href="site_libs/font-awesome-5.1.0/css/all.css" rel="stylesheet" />
<link href="site_libs/font-awesome-5.1.0/css/v4-shims.css" rel="stylesheet" />

<style type="text/css">code{white-space: pre;}</style>
<style type="text/css">
  pre:not([class]) {
    background-color: white;
  }
</style>
<script type="text/javascript">
if (window.hljs) {
  hljs.configure({languages: []});
  hljs.initHighlightingOnLoad();
  if (document.readyState && document.readyState === "complete") {
    window.setTimeout(function() { hljs.initHighlighting(); }, 0);
  }
}
</script>



<style type="text/css">
h1 {
  font-size: 34px;
}
h1.title {
  font-size: 38px;
}
h2 {
  font-size: 30px;
}
h3 {
  font-size: 24px;
}
h4 {
  font-size: 18px;
}
h5 {
  font-size: 16px;
}
h6 {
  font-size: 12px;
}
.table th:not([align]) {
  text-align: left;
}
</style>

<link rel="stylesheet" href="cm_yeti_bootswatch.css" type="text/css" />



<style type = "text/css">
.main-container {
  max-width: 940px;
  margin-left: auto;
  margin-right: auto;
}
code {
  color: inherit;
  background-color: rgba(0, 0, 0, 0.04);
}
img {
  max-width:100%;
}
.tabbed-pane {
  padding-top: 12px;
}
.html-widget {
  margin-bottom: 20px;
}
button.code-folding-btn:focus {
  outline: none;
}
summary {
  display: list-item;
}
</style>


<style type="text/css">
/* padding for bootstrap navbar */
body {
  padding-top: 66px;
  padding-bottom: 40px;
}
/* offset scroll position for anchor links (for fixed navbar)  */
.section h1 {
  padding-top: 71px;
  margin-top: -71px;
}
.section h2 {
  padding-top: 71px;
  margin-top: -71px;
}
.section h3 {
  padding-top: 71px;
  margin-top: -71px;
}
.section h4 {
  padding-top: 71px;
  margin-top: -71px;
}
.section h5 {
  padding-top: 71px;
  margin-top: -71px;
}
.section h6 {
  padding-top: 71px;
  margin-top: -71px;
}
.dropdown-submenu {
  position: relative;
}
.dropdown-submenu>.dropdown-menu {
  top: 0;
  left: 100%;
  margin-top: -6px;
  margin-left: -1px;
  border-radius: 0 6px 6px 6px;
}
.dropdown-submenu:hover>.dropdown-menu {
  display: block;
}
.dropdown-submenu>a:after {
  display: block;
  content: " ";
  float: right;
  width: 0;
  height: 0;
  border-color: transparent;
  border-style: solid;
  border-width: 5px 0 5px 5px;
  border-left-color: #cccccc;
  margin-top: 5px;
  margin-right: -10px;
}
.dropdown-submenu:hover>a:after {
  border-left-color: #ffffff;
}
.dropdown-submenu.pull-left {
  float: none;
}
.dropdown-submenu.pull-left>.dropdown-menu {
  left: -100%;
  margin-left: 10px;
  border-radius: 6px 0 6px 6px;
}
</style>

<script>
// manage active state of menu based on current page
$(document).ready(function () {
  // active menu anchor
  href = window.location.pathname
  href = href.substr(href.lastIndexOf('/') + 1)
  if (href === "")
    href = "index.html";
  var menuAnchor = $('a[href="' + href + '"]');

  // mark it active
  menuAnchor.parent().addClass('active');

  // if it's got a parent navbar menu mark it active as well
  menuAnchor.closest('li.dropdown').addClass('active');
});
</script>

<!-- tabsets -->

<style type="text/css">
.tabset-dropdown > .nav-tabs {
  display: inline-table;
  max-height: 500px;
  min-height: 44px;
  overflow-y: auto;
  background: white;
  border: 1px solid #ddd;
  border-radius: 4px;
}

.tabset-dropdown > .nav-tabs > li.active:before {
  content: "";
  font-family: 'Glyphicons Halflings';
  display: inline-block;
  padding: 10px;
  border-right: 1px solid #ddd;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li.active:before {
  content: "&#xe258;";
  border: none;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open:before {
  content: "";
  font-family: 'Glyphicons Halflings';
  display: inline-block;
  padding: 10px;
  border-right: 1px solid #ddd;
}

.tabset-dropdown > .nav-tabs > li.active {
  display: block;
}

.tabset-dropdown > .nav-tabs > li > a,
.tabset-dropdown > .nav-tabs > li > a:focus,
.tabset-dropdown > .nav-tabs > li > a:hover {
  border: none;
  display: inline-block;
  border-radius: 4px;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li {
  display: block;
  float: none;
}

.tabset-dropdown > .nav-tabs > li {
  display: none;
}
</style>

<!-- code folding -->
<style type="text/css">
.code-folding-btn { margin-bottom: 4px; }
</style>



<style type="text/css">

#TOC {
  margin: 25px 0px 20px 0px;
}
@media (max-width: 768px) {
#TOC {
  position: relative;
  width: 100%;
}
}


.toc-content {
  padding-left: 30px;
  padding-right: 40px;
}

div.main-container {
  max-width: 1200px;
}

div.tocify {
  width: 20%;
  max-width: 260px;
  max-height: 85%;
}

@media (min-width: 768px) and (max-width: 991px) {
  div.tocify {
    width: 25%;
  }
}

@media (max-width: 767px) {
  div.tocify {
    width: 100%;
    max-width: none;
  }
}

.tocify ul, .tocify li {
  line-height: 20px;
}

.tocify-subheader .tocify-item {
  font-size: 0.90em;
}

.tocify .list-group-item {
  border-radius: 0px;
}


</style>



</head>

<body>


<div class="container-fluid main-container">


<!-- setup 3col/9col grid for toc_float and main content  -->
<div class="row-fluid">
<div class="col-xs-12 col-sm-4 col-md-3">
<div id="TOC" class="tocify">
</div>
</div>

<div class="toc-content col-xs-12 col-sm-8 col-md-9">




<div class="navbar navbar-inverse  navbar-fixed-top" role="navigation">
  <div class="container">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar">
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <a class="navbar-brand" href="index.html">Ecology, Statistics, and Data Science with R</a>
    </div>
    <div id="navbar" class="navbar-collapse collapse">
      <ul class="nav navbar-nav">
        <li>
  <a href="index.html">Home</a>
</li>
<li>
  <a href="Head2_Schedule.html">Schedule</a>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-expanded="false">
    R
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
    <li>
      <a href="Head1_CourseContent.html">About Course Content</a>
    </li>
    <li class="divider"></li>
    <li class="dropdown-header">Module 1: Introduction to R</li>
    <li>
      <a href="00_CourseIntroductionFrame.html">1.0 Course Introduction</a>
    </li>
    <li>
      <a href="01_Rintro.html">1.1 First Steps</a>
    </li>
    <li>
      <a href="02_DataWrangling.html">1.2 Data Wrangling</a>
    </li>
    <li>
      <a href="03_Plotting.html">1.3 Plotting</a>
    </li>
    <li class="divider"></li>
    <li class="dropdown-header">Module 2: Spatial Analyses</li>
    <li>
      <a href="04_Spatial.html">2.1 Spatial Data</a>
    </li>
    <li>
      <a href="05_Raster.html">2.2 Spatial Raster Data</a>
    </li>
    <li>
      <a href="06_RasterTwo.html">2.3 More Raster Data</a>
    </li>
    <li class="divider"></li>
    <li class="dropdown-header">Module 3: Building R packages</li>
    <li>
      <a href="Quickstart_RPackages.html">3.1 Quickstart guide</a>
    </li>
    <li class="dropdown-header">Module 4: Reproducible and Interactive Research</li>
    <li>
      <a href="07_Reproducible.html">4.1 Research Reports with R markdown</a>
    </li>
    <li>
      <a href="11_Git.html">4.2 Version control with Github</a>
    </li>
    <li>
      <a href="07_03_RMarkdown_Tools.html">4.3 RMarkdown Tools: Interactive Presentations, Apps, Websites</a>
    </li>
    <li class="divider"></li>
  </ul>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-expanded="false">
    Ecological Modeling
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
    <li>
      <a href="101SDMs.html">Introduction to Species Distribution Models</a>
    </li>
    <li>
      <a href="4_1_Wallace_SDMs.html">Range Models with Wallace</a>
    </li>
    <li>
      <a href="20_Intro_Demography.html">Introduction to Demography</a>
    </li>
    <li>
      <a href="21_Intro_IPMs.html">Introduction to Integral Projection Models</a>
    </li>
    <li class="divider"></li>
    <li class="dropdown-header">Ecoinformatics Workshop</li>
    <li>
      <a href="3_1_Ecoinf_Schedule.html">0 Workshop Schedule</a>
    </li>
    <li>
      <a href="3_8_BIEN_intro.html">1 (BIEN) Botanical Information and Ecology Network</a>
    </li>
    <li>
      <a href="3_7_TNRS.html">2 Taxonomic Name Resolution</a>
    </li>
    <li>
      <a href="3_3_RBIEN_tutorial.html">3 Accessing Plant Biodiversity Data</a>
    </li>
    <li>
      <a href="3_4_wallace.html">4 A streamlined modeling workflow: Wallace</a>
    </li>
    <li>
      <a href="3_5_Minxent.html">5 Integrating biogeographic data </a>
    </li>
    <li>
      <a href="3_2_Networks.html">6 Network Associations</a>
    </li>
    <li>
      <a href="3_6_Teaching_Ecoinformatics.html">7 Teaching Econformatics</a>
    </li>
  </ul>
</li>
<li>
  <a href="Head3_Resources.html">Resources</a>
</li>
      </ul>
      <ul class="nav navbar-nav navbar-right">
        <li>
  <a href="https://github.com/cmerow/RDataScience/tree/gh-pages">
    <span class="fa fa-github fa-lg"></span>
     
  </a>
</li>
      </ul>
    </div><!--/.nav-collapse -->
  </div><!--/.container -->
</div><!--/.navbar -->

<div class="fluid-row" id="header">

<div class="btn-group pull-right">
<button type="button" class="btn btn-default btn-xs dropdown-toggle" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false"><span>Code</span> <span class="caret"></span></button>
<ul class="dropdown-menu" style="min-width: 50px;">
<li><a id="rmd-show-all-code" href="#">Show All Code</a></li>
<li><a id="rmd-hide-all-code" href="#">Hide All Code</a></li>
</ul>
</div>



<h1 class="title toc-ignore">Introduction to Species Distribution Modeling</h1>
<h4 class="author"><a href="cmerow.github.io">Cory Merow</a></h4>

</div>


<!-- <div> -->
<!-- <iframe src="05_presentation/05_Spatial.html" width="100%" height="700px"> </iframe> -->
<!-- </div> -->
<div>
<object data="101_assets/SDM101_Intro.pdf" type="application/pdf" width="100%" height="600px">
<p>
It appears you don’t have a PDF plugin for this browser. No biggie… you can <a href="02_assets/02_DataWrangling.pdf">click here to download the PDF file.</a>
</p>
</object>
</div>
<p>
<a href="101_assets/SDM101_Intro.pdf">Download the PDF of the presentation</a>
</p>
<p><a href="101SDMs.R"><i class="fa fa-file-code-o fa-3x" aria-hidden="true"></i> The R Script associated with this page is available here</a>. Download this file and open it (or copy-paste into a new script) with RStudio so you can follow along.</p>
<div id="setup" class="section level1">
<h1><span class="header-section-number">1</span> Setup</h1>
<pre class="r"><code>library(spocc)
library(raster)</code></pre>
<pre><code>## Loading required package: sp</code></pre>
<pre class="r"><code>library(sp)
library(rgdal)</code></pre>
<pre><code>## rgdal: version: 1.4-4, (SVN revision 833)
##  Geospatial Data Abstraction Library extensions to R successfully loaded
##  Loaded GDAL runtime: GDAL 2.1.3, released 2017/20/01
##  Path to GDAL shared files: /Library/Frameworks/R.framework/Versions/3.6/Resources/library/rgdal/gdal
##  GDAL binary built with GEOS: FALSE 
##  Loaded PROJ.4 runtime: Rel. 4.9.3, 15 August 2016, [PJ_VERSION: 493]
##  Path to PROJ.4 shared files: /Library/Frameworks/R.framework/Versions/3.6/Resources/library/rgdal/proj
##  Linking to sp version: 1.3-1</code></pre>
<pre class="r"><code>library(ROCR)</code></pre>
<pre><code>## Loading required package: gplots</code></pre>
<pre><code>## 
## Attaching package: &#39;gplots&#39;</code></pre>
<pre><code>## The following object is masked from &#39;package:stats&#39;:
## 
##     lowess</code></pre>
<pre class="r"><code>library(corrplot)</code></pre>
<pre><code>## corrplot 0.84 loaded</code></pre>
<pre class="r"><code>library(maxnet)
library(spThin)</code></pre>
<pre><code>## Loading required package: spam</code></pre>
<pre><code>## Loading required package: dotCall64</code></pre>
<pre><code>## Loading required package: grid</code></pre>
<pre><code>## Spam version 2.2-2 (2019-03-07) is loaded.
## Type &#39;help( Spam)&#39; or &#39;demo( spam)&#39; for a short introduction 
## and overview of this package.
## Help for individual functions is also obtained by adding the
## suffix &#39;.spam&#39; to the function name, e.g. &#39;help( chol.spam)&#39;.</code></pre>
<pre><code>## 
## Attaching package: &#39;spam&#39;</code></pre>
<pre><code>## The following objects are masked from &#39;package:base&#39;:
## 
##     backsolve, forwardsolve</code></pre>
<pre><code>## Loading required package: fields</code></pre>
<pre><code>## Loading required package: maps</code></pre>
<pre><code>## See https://github.com/NCAR/Fields for
##  an extensive vignette, other supplements and source code</code></pre>
<pre><code>## Loading required package: knitr</code></pre>
</div>
<div id="the-worst-sdm-ever" class="section level1">
<h1><span class="header-section-number">2</span> The worst SDM ever</h1>
<p>The goal of this section is to use the simplest possible set of operations to build an SDM. There are many packages that will perform much more refined versions of these steps, at the expense that decisions are made behind the scenes, or may be obscure to the user. So before getting into fancier tools, let’s see what the bare minimum looks like.</p>
<blockquote>
<p>This is not the simplest possible code, because it requires some familiarity with the internal components of different spatial objects. The tradeoff is that none of the key operations are performed behind the scenes by specialized SDM functions. I realize this is not always pretty, but I hope for that reason it can demonstrate some coding gynmastics for beginners.</p>
</blockquote>
<div id="get-presence-data" class="section level2">
<h2><span class="header-section-number">2.1</span> Get presence data</h2>
<p>The <code>spocc</code> package allows you to hit a number of the larger databases for presence-only data within R. They provide a number of useful pieces of metadata, if your’e into that sort of this. For this, we’re not; we just want lat and lon.</p>
<blockquote>
<p>Decision: You assume the database of choice has sufficiently checked for errors in biology or typos. You know what happens when you assume…</p>
</blockquote>
<!-- ```{r} -->
<!-- # get presence data -->
<!-- # pres=spocc::occ('Alliaria petiolata',from='gbif',limit=5000) # this can be slow -->
<!--   # so just read in the result of me running this earlier -->
<!-- pres=read.csv('https://cmerow.github.io/RDataScience/101_assets/AP_gbif.csv')[,c('longitude','latitude')] -->
<!-- pres=pres[complete.cases(pres),] # toss records without coords -->
<!-- ``` -->
<!-- ## Get environmental data -->
<!-- The `raster` package has a convenience function to get some types of data. To see more about [Worldclim](http://worldclim.org/version2) -->
<!-- > Decision: Worldclim data describes the environmental well in this region. The 'bioclim' variables are biologically relevant summaries of climate. -->
<!-- ```{r} -->
<!-- # get climate data -->
<!--   # the raster package has convenience function built in for worldclim -->
<!-- clim=getData('worldclim', var='bio', res=10) -->
<!-- ``` -->
<!-- The Bioclim variables in `clim.us` are: -->
<!-- <small> -->
<!-- Variable      Description -->
<!-- -    - -->
<!-- BIO1          Annual Mean Temperature -->
<!-- BIO2          Mean Diurnal Range (Mean of monthly (max temp – min temp)) -->
<!-- BIO3          Isothermality (BIO2/BIO7) (* 100) -->
<!-- BIO4          Temperature Seasonality (standard deviation *100) -->
<!-- BIO5          Max Temperature of Warmest Month -->
<!-- BIO6          Min Temperature of Coldest Month -->
<!-- BIO7          Temperature Annual Range (BIO5-BIO6) -->
<!-- BIO8          Mean Temperature of Wettest Quarter -->
<!-- BIO9          Mean Temperature of Driest Quarter -->
<!-- BIO10         Mean Temperature of Warmest Quarter -->
<!-- BIO11         Mean Temperature of Coldest Quarter -->
<!-- BIO12         Annual Precipitation -->
<!-- BIO13         Precipitation of Wettest Month -->
<!-- BIO14         Precipitation of Driest Month -->
<!-- BIO15         Precipitation Seasonality (Coefficient of Variation) -->
<!-- BIO16         Precipitation of Wettest Quarter -->
<!-- BIO17         Precipitation of Driest Quarter -->
<!-- BIO18         Precipitation of Warmest Quarter -->
<!-- BIO19         Precipitation of Coldest Quarter -->
<!-- </small> -->
<!-- ##  Choose domain -->
<!-- The 'domain' is the region of interest. It can be a political region, a biome, a park, a watershed, etc. It should include locations where the species is present and absent. Choosing relevent locations were the species does not occur is part of the art of *presence-only* modeling (see slides above). -->
<!-- > Decision: We are only asking about invasion in New England, so we constrain the domain to a bounding box around New England -->
<!-- ```{r} -->
<!-- # choose domain (just the Eastern US) -->
<!-- clim.us=raster::crop(clim,c(-76,-65,40,50)) # trim to a smaller region -->
<!-- plot(clim.us[[1]]) # plot just the 1st variable to see domain -->
<!-- ``` -->
<!-- ##  Prep data -->
<!-- Many climate variables are highly correlated with one another, which can confound statistical analyses. -->
<!-- > Decision: Correlated predictors can make it difficult to interpret model coefficients or response curves. So we'll remove the most correlated predictores -->
<!-- ```{r} -->
<!-- # check for correlated predictors -->
<!-- cors=cor(values(clim.us),use='complete.obs') # evaluate correlations -->
<!-- corrplot(cors,order = "AOE", addCoef.col = "grey",number.cex=.6) # plot correlations -->
<!-- ``` -->
<!-- This plot nicely clumps groups of similar variables. Choose a representative variable from each clump. -->
<!-- ```{r} -->
<!-- clim=clim[[c("bio1","bio2","bio13","bio14")]] # keep just reasonably uncorrelated ones -->
<!-- clim.us=clim.us[[c('bio1','bio2','bio13','bio14')]] # keep just reasonably uncorrelated ones -->
<!-- cors=cor(values(clim.us),use='complete.obs') # evaluate correlations -->
<!-- corrplot(cors,order = "AOE", addCoef.col = "grey",number.cex=.6)# plot correlations -->
<!-- ``` -->
<!-- Ok, tolerable. Some people advocate that correlations should be <0.7. I prefer lower, like 0.3, or 0.4 because I often forecast (as we'll do below) and one must assume that those correlations hold in new scenarios hold to make meaningful forecasts. -->
<!-- Scaling each predictor to zero mean and unit variance is a common statistical approach to make sure the coefficents you'll estimate are comparable (on the same scale) and prevents a few other wonky things from possibly happening. -->
<!-- ```{r} -->
<!-- # scale each predictor to mean=0, variance=1 -->
<!-- clim.means=apply(values(clim.us),2,mean,na.rm=T) # means -->
<!-- clim.sds=apply(values(clim.us),2,sd,na.rm=T) # standard devations -->
<!-- name=names(clim.us) -->
<!-- values(clim.us)=sapply(1:nlayers(clim.us),function(x) (values(clim.us)[,x]-clim.means[x])/clim.sds[x])  -->
<!-- # z-scores -->
<!-- names(clim.us)=name -->
<!-- # get environment at pres points -->
<!-- coordinates(pres)=c('longitude','latitude') # set coords to allow extraction (next line) -->
<!-- pres.data=data.frame(raster::extract(clim.us,pres)) # extract data at pres locations -->
<!-- coordinates(pres.data)=coordinates(pres) # make sure the data have coords associated -->
<!-- pres.data=pres.data[complete.cases(pres.data@data),] # toss points without env data -->
<!-- ``` -->
<!-- ```{r} -->
<!-- plot(clim.us) # view  -->
<!-- ``` -->
<!-- ##  Sample background -->
<!-- In presence-only (PO) modeling, where absence data do not exist, so-called 'background' (==jargon) points are used. In PO models, one compares the environmental conditions at occupied locations (presences) to the conditions available in the region of interest. This corresponds to 'use-avialability' (==jargon) analysis and asks, 'how much does the species use environment x in proportion to its availability?' For example, if a landscape contains 10 cells with temperature < 20 degrees, and the species uses all of them, you would infer that cold locations are important (10 are used and 10 are available). In contrast, if the lanscape had 1000 cells with temperature <20 degrees, you'd infer the opposite, that cold cells are probably avoided (10 are used and 1000 are available). This is the essence of (this type of) presence-only modeling. -->
<!-- > Decision: The species is equally likely to be anywhere on the landscapes, so we'll compare presences to a random sample of background points. -->
<!-- (There's a lot of subtlties about background selection, just go with it for now...) -->
<!-- ```{r} -->
<!--    ## save the data table -->
<!-- # sample background (to compare against presences) -->
<!-- all.background=which(complete.cases(values(clim.us))) # find cells on land -->
<!-- bg.index=sample(all.background,min(length(all.background),10000)) # take random sample of land -->
<!-- bg.data=data.frame(values(clim.us)[bg.index,]) # get the env at these cells -->
<!-- coordinates(bg.data)=coordinates(clim.us)[bg.index,] # define spatial object -->
<!-- ``` -->
<!-- ## Statistical model -->
<!-- > Decision: Linear and quadratic terms are sufficient to describe the species' response to the environment.  -->
<!-- Next, combine the data into a convenient form and specify a formula for the regression. -->
<!-- ```{r} -->
<!-- # prep data for use in glm() -->
<!-- all.data=rbind(data.frame(pres=1,pres.data@data),data.frame(pres=0,bg.data@data)) # line up pres & bg -->
<!-- # specify formula (quickly to avoid writing out every name) -->
<!-- (form=paste('pres/weight~', # lhs of eqn. -->
<!--             paste(names(all.data)[-1], collapse = " + "),'+', # linear terms -->
<!--             paste("I(", names(all.data)[-1], "^2)", sep = "", collapse = " + "))) # qudratic terms -->
<!-- ``` -->
<!-- There are some subtle differences here compared to a regular old GLM. These weights allow one to fit a Poisson point process model with the `glm` function. If this sort of thing excites you, [this paper](http://onlinelibrary.wiley.com/doi/10.1111/2041-210X.12352/abstract) -->
<!-- describes point process models well, and the appendix describes this weighting scheme. If it doesn't, just pretend this is a regular GLM for now.  -->
<!-- ```{r} -->
<!-- all.data$weight = all.data$pres + (1 - all.data$pres) * 10000 # these allow you to fit a Point Process -->
<!-- mod.worst=glm(form,data=all.data,family=poisson(link='log'),weights=weight) # fit the model -->
<!-- summary(mod.worst) # show coefficients -->
<!-- ``` -->
<!-- ## Inspect response curves -->
<!-- Response curves describe how the species' occurrence (y-axis) depends on a single climate variable (x-axis). There's one for each environmental variable in the model. Usually this is done by making predictions with all the other predictors set at their means. Most packages have 1-liners to make this instead, but this builds more character. -->
<!-- ```{r,results='hide'} -->
<!-- # check response curves -->
<!--   # these marginal response curves are evaluated at the means of the non-focal predictor -->
<!-- clim.ranges=apply(values(clim.us),2,range,na.rm=T) # upper and lower limits for each variable -->
<!-- dummy.mean.matrix=data.frame(matrix(0,ncol=nlayers(clim.us),nrow=100)) #makes prediction concise below -->
<!-- names(dummy.mean.matrix)=colnames(clim.ranges) # line up names for later reference -->
<!-- response.curves=lapply(1:nlayers(clim.us),function(x){ # loop over each variable -->
<!--   xs=seq(clim.ranges[1,x],clim.ranges[2,x],length=100) # x values to evaluate the curve -->
<!--   newdata=dummy.mean.matrix # data frame with right structure -->
<!--   newdata[,x]=xs # plug in just the values for the focal variable that differ from mean -->
<!--   ys=predict(mod.worst,newdata=newdata) # predictions -->
<!--   return(data.frame(xs=xs,ys=ys)) # define outputs -->
<!-- })# ignore warnings -->
<!-- ``` -->
<!-- Check out the list of lists that store this. -->
<!-- ```{r} -->
<!-- str(response.curves) #structure of the object used for plotting -->
<!-- ``` -->
<!-- ```{r,width=7} -->
<!--   # plot the curves -->
<!-- par(mfrow=c(2,2),mar=c(4,5,.5,.5)) # # rows and cols for plotting -->
<!-- for(i in 1:nlayers(clim.us)){ # loop over layers -->
<!--   plot(response.curves[[i]]$xs,response.curves[[i]]$ys, # xs and ys -->
<!--        type='l', # line plot -->
<!--        bty='n',las=1, # decorations -->
<!--        ylim=c(-20,20), # y axis limits -->
<!--        xlab=colnames(clim.ranges)[i],ylab='occurence rate') # axis labels -->
<!--   pres.env.range=range(pres.data[names(clim.us)[i]]@data)  # find limits of fitting data -->
<!--   abline(v=pres.env.range,col='red',lty=2)  # plot limits of fitting data -->
<!-- } -->
<!-- ``` -->
<!-- ## Map predictions -->
<!-- > Decision: When predicting, its ok to extrapolate beyond  -->
<!-- ```{r, width=7} -->
<!-- # predict to US -->
<!-- pred.r=raster::predict(clim.us,mod.worst, index=1,type="response") -->
<!-- pred.r=pred.r/sum(values(pred.r),na.rm=T) # normalize prediction (sum to 1) -->
<!-- plot(log(pred.r)) # plot raster -->
<!-- plot(pres,add=T) # plot points -->
<!-- ``` -->
<!-- ## Evaluate performance -->
<!-- Reciever-operator characteristic (ROC) curves are often used to evaluation binary (presence/absence) predictions. Since the predictions are continuous (see previous map), we need to choose a threshold that distinguishes presence from absence. The ROC curve summarizes the results for all possible thresholds. Each point corresponds to a threshold, the y-axis describes the proportion of presences correctly predicted while the x-axis describes the proportion of background points where presence is predicted. There's a clear tradeoff in getting a lot of presences right without predicting presence everywhere. The AUC (area under the curve) describes the area under the ROC curve - a value of 1 is the best, and a value of 0.5 means you may as well flip a coin. More [here](https://en.wikipedia.org/wiki/Receiver_operating_characteristic). -->
<!-- ```{r} -->
<!-- # evaluate -->
<!-- pred.at.fitting.pres=raster::extract(pred.r,pres.data) # get predictions at pres locations -->
<!-- pred.at.fitting.bg=raster::extract(pred.r,bg.data) # get predictions at background locations -->
<!-- rocr.pred=ROCR::prediction(predictions=c(pred.at.fitting.pres,pred.at.fitting.bg), -->
<!--                           labels=c(rep(1,length(pred.at.fitting.pres)),rep(0,length(pred.at.fitting.bg)))) # define the prediction object needed by ROCR -->
<!-- perf.fit=performance(rocr.pred,measure = "tpr", x.measure = "fpr") # calculate perfomance  -->
<!-- plot(perf.fit) # plot ROC curve -->
<!-- abline(0,1) # 1:1 line indicate random predictions  -->
<!-- (auc_ROCR <- performance(rocr.pred, measure = "auc")@y.values[[1]]) # get AUC -->
<!-- ``` -->
<!-- ## Transfer to new conditions -->
<!-- A common goal of SDMing is to transfer the models to new locations. -->
<!-- > Decision: The occurrence-environment relationship fit in New England also describes the species response to environment in Europe. -->
<!-- ```{r,width=7} -->
<!-- # transfer to Europe -->
<!-- # choose domain (just europe) -->
<!-- clim.eu=raster::crop(clim,c(-10,55,30,75)) -->
<!-- values(clim.eu)=sapply(1:nlayers(clim.eu),function(x) (values(clim.eu)[,x]-clim.means[x])/clim.sds[x]) -->
<!-- names(clim.eu)=names(clim.us) -->
<!-- # z-scores (to make values comparable to the scaeld values for fitting) -->
<!-- transfer.r=raster::predict(clim.eu,mod.worst, index=1,type="response") -->
<!-- transfer.r=transfer.r/sum(values(transfer.r),na.rm=T) # normalize prediction (sum to 1) -->
<!-- plot(log(transfer.r)) # plot preds -->
<!-- plot(pres,add=T) # plot presences  -->
<!-- ``` -->
<!-- # Improvements -->
<!-- <!-- #========================================================================= -->
<p>–&gt; <!-- ## Sampling bias --></p>
<!-- ###  Sample background -->
<!-- <img src="101_assets/Phillips_bias.png" style="width: 100"/> -->
<!-- > Decision: Presences are most likely to be observed where other specie sampled with the same protocol or are taxonomically similar were sampled.  -->
<!-- The data in `bias.bg` are the result of extracting the coordinates of all 187 species observed in the Invasive Plant Atlas of New England (IPANE) data base. -->
<!-- ```{r} -->
<!-- bias.bg=read.csv('https://cmerow.github.io/YaleBGCCourses/101_assets/Bias_IPANE_allPoints.csv')[,-1] -->
<!-- coordinates(bias.bg)=c(1,2) -->
<!-- bias.bg.data=data.frame(raster::extract(clim.us,bias.bg)) -->
<!-- coordinates(bias.bg.data)=coordinates(bias.bg) -->
<!-- bias.bg.data=bias.bg.data[complete.cases(bias.bg.data@data),] -->
<!-- ``` -->
<!-- ```{r} -->
<!-- # prep data for use in glm() -->
<!-- all.data=rbind(data.frame(pres=1,pres.data@data),data.frame(pres=0,bias.bg.data@data)) -->
<!-- # specify formula (quickly to avoid writing out every name) -->
<!-- (form=paste('pres/weight~', # lhs of eqn. -->
<!--             paste(names(all.data)[-1], collapse = " + "),'+', # linear terms -->
<!--             paste("I(", names(all.data)[-1], "^2)", sep = "", collapse = " + "))) # qudratic terms -->
<!-- ``` -->
<!-- *From this point on, the code is exactly the same as the previous example, except that the model is given a new name, `mod.bias` instead of `mod.worst`* -->
<!-- ## Statistical model -->
<!-- ```{r} -->
<!-- all.data$weight = all.data$pres + (1 - all.data$pres) * 10000 # these allow you to fit a Point Process -->
<!-- mod.bias=glm(form,data=all.data,family=poisson(link='log'),weights=weight) # fit the model -->
<!-- summary(mod.bias) # show coefficients -->
<!-- ``` -->
<!-- ## Inspect response curves -->
<!-- ```{r,results='hide'} -->
<!-- # check response curves -->
<!--   # these marginal response curves are evaluated at the means of the non-focal predictor -->
<!-- clim.ranges=apply(values(clim.us),2,range,na.rm=T) # upper and lower limits for each variable -->
<!-- dummy.mean.matrix=data.frame(matrix(0,ncol=nlayers(clim.us),nrow=100)) #makes prediction concise below -->
<!-- names(dummy.mean.matrix)=colnames(clim.ranges) # line up names for later reference -->
<!-- response.curves=lapply(1:nlayers(clim.us),function(x){ # loop over each variable -->
<!--   xs=seq(clim.ranges[1,x],clim.ranges[2,x],length=100) # x values to evaluate the curve -->
<!--   newdata=dummy.mean.matrix # data frame with right structure -->
<!--   newdata[,x]=xs # plug in just the values for the focal variable that differ from mean -->
<!--   ys=predict(mod.bias,newdata=newdata) # predictions -->
<!--   return(data.frame(xs=xs,ys=ys)) # define outputs -->
<!-- })# ignore warnings -->
<!-- ``` -->
<!-- ```{r} -->
<!-- str(response.curves) #structure of the object used for plotting -->
<!-- ``` -->
<!-- ```{r, width=7} -->
<!--   # plot the curves -->
<!-- par(mfrow=c(2,2),mar=c(4,5,.5,.5)) # # rows and cols for plotting -->
<!-- for(i in 1:nlayers(clim.us)){ # loop over layers -->
<!--   plot(response.curves[[i]]$xs,response.curves[[i]]$ys, -->
<!--        type='l',bty='n',las=1,xlab=colnames(clim.ranges)[i],ylab='occurence rate',ylim=c(-20,20)) -->
<!--   pres.env.range=range(pres.data[names(clim.us)[i]]@data) # find limits of fitting data -->
<!--   abline(v=pres.env.range,col='red',lty=2) # plot limits of fitting data -->
<!-- } -->
<!-- ``` -->
<!-- ### Map predictions -->
<!-- ```{r, width=7} -->
<!-- # predict to US -->
<!-- pred.r=raster::predict(clim.us,mod.bias, index=1,type="response") -->
<!-- pred.r=pred.r/sum(values(pred.r),na.rm=T) # normalize prediction (sum to 1) -->
<!-- plot(log(pred.r)) # plot raster -->
<!-- plot(pres,add=T) # plot points -->
<!-- ``` -->
<!-- ### Evaluate performance -->
<!-- ```{r,width=7} -->
<!-- # evaluate -->
<!-- pred.at.fitting.pres=raster::extract(pred.r,pres.data) # get predictions at pres locations -->
<!-- pred.at.fitting.bg=raster::extract(pred.r,bg.data) # get predictions at background locations -->
<!-- rocr.pred=ROCR::prediction(predictions=c(pred.at.fitting.pres,pred.at.fitting.bg), -->
<!--                           labels=c(rep(1,length(pred.at.fitting.pres)),rep(0,length(pred.at.fitting.bg)))) # define the prediction object needed by ROCR -->
<!-- perf.fit=performance(rocr.pred,measure = "tpr", x.measure = "fpr") # calculate perfomance  -->
<!-- plot(perf.fit) # plot ROC curve -->
<!-- abline(0,1) # 1:1 line indicate random predictions  -->
<!-- (auc_ROCR <- performance(rocr.pred, measure = "auc")@y.values[[1]]) # get AUC -->
<!-- ``` -->
<!-- Oh, Snap! Not nearly as good as the model that ignored bias. This is common when accounting for bias; you've removed something that structured the observations from the model. So you evaluation on the *fitting* data will be worse. Performance on new data sets will often be better (conditional on the rest of the model being well designed), so long as those don't suffer from the same sampling bias patterns. -->
<!-- <!-- #========================================================================= -->
<p>–&gt; <!-- <!-- #========================================================================= --> –&gt; <!-- ## Other algorithms: glmnet --></p>
<!-- ## Statistical model -->
<!-- ```{r} -->
<!-- mod.maxnet=maxnet(p=all.data[,'pres'],data=all.data[,c("bio1","bio2","bio13","bio14")]) -->
<!-- summary(mod.maxnet) # show coefficients -->
<!-- ``` -->
<!-- *From this point on, the code is exactly the same as the previous example, except that the model is given a new name, `mod.maxnet` instead of `mod.bias`* -->
<!-- ## Inspect response curves -->
<!-- ```{r,results='hide'} -->
<!-- # check response curves -->
<!--   # these marginal response curves are evaluated at the means of the non-focal predictor -->
<!-- clim.ranges=apply(values(clim.us),2,range,na.rm=T) # upper and lower limits for each variable -->
<!-- dummy.mean.matrix=data.frame(matrix(0,ncol=nlayers(clim.us),nrow=100)) #makes prediction concise below -->
<!-- names(dummy.mean.matrix)=colnames(clim.ranges) # line up names for later reference -->
<!-- response.curves=lapply(1:nlayers(clim.us),function(x){ # loop over each variable -->
<!--   xs=seq(clim.ranges[1,x],clim.ranges[2,x],length=100) # x values to evaluate the curve -->
<!--   newdata=dummy.mean.matrix # data frame with right structure -->
<!--   newdata[,x]=xs # plug in just the values for the focal variable that differ from mean -->
<!--   ys=predict(mod.maxnet,newdata=newdata) # predictions -->
<!--   return(data.frame(xs=xs,ys=ys)) # define outputs -->
<!-- })# ignore warnings -->
<!-- ``` -->
<!-- ```{r} -->
<!--   # plot the curves -->
<!-- par(mfrow=c(2,2),mar=c(4,5,.5,.5)) # # rows and cols for plotting -->
<!-- for(i in 1:nlayers(clim.us)){ # loop over layers -->
<!--   plot(response.curves[[i]]$xs,response.curves[[i]]$ys, -->
<!--        type='l',bty='n',las=1,xlab=colnames(clim.ranges)[i],ylab='occurence rate',ylim=c(-20,20)) -->
<!--   pres.env.range=range(pres.data[names(clim.us)[i]]@data)  # find limits of fitting data -->
<!--   abline(v=pres.env.range,col='red',lty=2)  # plot limits of fitting data -->
<!-- } -->
<!-- ``` -->
<!-- ### Map predictions -->
<!-- ```{r} -->
<!-- # predict to US -->
<!-- pred.r=raster::predict(clim.us,mod.maxnet, index=1,type="exponential") # note 'type' differs from glm -->
<!-- pred.r=pred.r/sum(values(pred.r),na.rm=T) # normalize prediction (sum to 1) -->
<!-- plot(log(pred.r)) # plot raster -->
<!-- plot(pres,add=T) # plot points -->
<!-- ``` -->
<!-- ### Evaluate performance -->
<!-- ```{r} -->
<!-- # evaluate -->
<!-- pred.at.fitting.pres=raster::extract(pred.r,pres.data) # get predictions at pres locations -->
<!-- pred.at.fitting.bg=raster::extract(pred.r,bg.data) # get predictions at background locations -->
<!-- rocr.pred=ROCR::prediction(predictions=c(pred.at.fitting.pres,pred.at.fitting.bg), -->
<!--                           labels=c(rep(1,length(pred.at.fitting.pres)),rep(0,length(pred.at.fitting.bg)))) # define the prediction object needed by ROCR -->
<!-- perf.fit=performance(rocr.pred,measure = "tpr", x.measure = "fpr") # calculate perfomance  -->
<!-- plot(perf.fit) # plot ROC curve -->
<!-- abline(0,1) # 1:1 line indicate random predictions  -->
<!-- (auc_ROCR <- performance(rocr.pred, measure = "auc")@y.values[[1]]) # get AUC -->
<!-- ``` -->
<!-- Great, we've recovered some of the predictive accuracy lost when the sampling bias was factored out. By picking up more complex responses, we're better able to describe the distribution. An important caveat is that we should check for overfitting, wherein we've fit to idiosyncracies of the particular fitting data set. To check this, we'd need to evaluate on independent data, which we won't get to here *but is a critical step on any SDM you intend to publish.* -->
<!-- ## Other options -->
<!-- Within the framework described above: -->
<!-- * Thin presences to remove spatial autocorrelation -->
<!-- * Subsample presences to evaluate model on independent data (e.g., k-fold cross validation) -->
<!-- * Use remotely sensed data (removing artefacts of interpolation) -->
<!-- * More informative performance statistics (Boyce, partial AUC) -->
<!-- * Other algorithms (GAMs, Tree-based methods, Envelope methods) -->
<!-- * Model selection to find better suites of predictors -->
<!-- Somewhat different frameworks: -->
<!-- * Model space explicitly (accounting for spatial autocorrelation) -->
<!-- * Borrow strength from other species (Joint SDMs) -->
<!-- * Bayesian models for more complete treatment of uncertainty or hierarchical structure, among other things -->
<!-- * Ensemble approaches to combine the results of multiple models -->
<!-- * Combine data from different parts of the distribution (e.g. native range) -->
<!-- <!-- #========================================================================= -->
<p>–&gt; <!-- <!-- ## Thin presences, Stratify sampling --> –&gt;</p>
<!-- <!-- ```{r} -->
<p>–&gt; <!-- <!-- all.data$weight = all.data$pres + (1 - all.data$pres) * 10000 # these allow you to fit a Point Process --> –&gt; <!-- <!-- mod.worst=glm(form,data=all.data,family=poisson(link='log'),weights=weight) # fit the model --> –&gt; <!-- <!-- #mod.worst=maxnet(all.data$pres,all.data[-1]) --> –&gt; <!-- <!-- summary(mod.worst) # show coefficients --> –&gt; <!-- <!-- ``` --> –&gt;</p>
<!-- <!-- ## Model Comparison -->
<p>–&gt; <!-- <!-- # # evaluate transfer --> –&gt; <!-- <!-- # pred.at.transfer.pres=raster::extract(transfer.r,pres.data) --> –&gt; <!-- <!-- #   # sample background in transfer region --> –&gt; <!-- <!-- # all.background=which(complete.cases(values(clim.us))) --> –&gt; <!-- <!-- # bg.index=sample(all.background,10000) --> –&gt; <!-- <!-- # bg.data=data.frame(values(clim.us)[bg.index,]) --> –&gt; <!-- <!-- # coordinates(bg.data)=coordinates(clim.us)[bg.index,] --> –&gt; <!-- <!-- #  --> –&gt; <!-- <!-- # transfer.bg= --> –&gt; <!-- <!-- # pred.at.fitting.bg=raster::extract(transfer.r,bg.data) --> –&gt; <!-- <!-- # rocr.pred=ROCR::prediction(predictions=c(pred.at.fitting.pres,pred.at.fitting.bg), --> –&gt; <!-- <!-- #                           labels=c(rep(1,length(pred.at.fitting.pres)),rep(0,length(pred.at.fitting.bg)))) --> –&gt; <!-- <!-- # perf.fit=performance(rocr.pred,measure = "tpr", x.measure = "fpr") --> –&gt; <!-- <!-- # plot(perf.fit) --> –&gt; <!-- <!-- # abline(0,1) --> –&gt; <!-- <!-- # (auc_ROCR <- performance(rocr.pred, measure = "auc")@y.values[[1]]) --> –&gt; <!-- <!-- #  --> –&gt;</p>
</div>
</div>


<!-- give the footer some space -->
<br/>
<br/>

<footer id="site-footer">
  <div id="footer1">
  <a href="https://cmerow.github.io"><img src="img/cory.png" alt="logo" width=40px></a>
  <!--
  <a href="http://adamwilson.us/#contact"><i class="fa fa-envelope fa-2x"></i></a> 
  <a href="https://twitter.com/AdamWilsonLab"><i class="fa fa-twitter fa-2x"></i></a> 
  <a href="https://github.com/AdamMWilson"><i class="fa fa-github fa-2x"></i></a>
  -->
  </div>
  <div id="footer2">
  <a rel="license" property="http://creativecommons.org/ns#license"
  href="http://creativecommons.org/licenses/by/4.0/" ><img src="img/cc-by.svg" alt="cc-by"/></a> 
  </div>
</footer>


</div>
</div>

</div>

<script>

// add bootstrap table styles to pandoc tables
function bootstrapStylePandocTables() {
  $('tr.header').parent('thead').parent('table').addClass('table table-condensed');
}
$(document).ready(function () {
  bootstrapStylePandocTables();
});


</script>

<!-- tabsets -->

<script>
$(document).ready(function () {
  window.buildTabsets("TOC");
});

$(document).ready(function () {
  $('.tabset-dropdown > .nav-tabs > li').click(function () {
    $(this).parent().toggleClass('nav-tabs-open')
  });
});
</script>

<!-- code folding -->
<script>
$(document).ready(function () {
  window.initializeCodeFolding("show" === "show");
});
</script>

<script>
$(document).ready(function ()  {

    // move toc-ignore selectors from section div to header
    $('div.section.toc-ignore')
        .removeClass('toc-ignore')
        .children('h1,h2,h3,h4,h5').addClass('toc-ignore');

    // establish options
    var options = {
      selectors: "h1,h2,h3",
      theme: "bootstrap3",
      context: '.toc-content',
      hashGenerator: function (text) {
        return text.replace(/[.\\/?&!#<>]/g, '').replace(/\s/g, '_').toLowerCase();
      },
      ignoreSelector: ".toc-ignore",
      scrollTo: 0
    };
    options.showAndHide = true;
    options.smoothScroll = true;

    // tocify
    var toc = $("#TOC").tocify(options).data("toc-tocify");
});
</script>


</body>
</html>
